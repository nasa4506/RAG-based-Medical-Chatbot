# Medical Chatbot - Complete Workflow Documentation ğŸ”„

This document explains the complete end-to-end workflow of how the Medical Chatbot processes documents and answers questions.

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture Components](#architecture-components)
3. [Workflow Phases](#workflow-phases)
4. [Detailed Component Breakdown](#detailed-component-breakdown)
5. [Data Flow Diagrams](#data-flow-diagrams)
6. [Code Execution Flow](#code-execution-flow)

---

## ğŸ¯ Overview

The Medical Chatbot uses **Retrieval Augmented Generation (RAG)** to answer medical questions by:
1. Processing PDF documents and storing them as embeddings
2. Searching for relevant context when questions are asked
3. Using AI to generate answers based on retrieved context

**Key Technologies:**
- **LangChain**: Orchestrates the RAG pipeline
- **Pinecone**: Vector database for semantic search
- **HuggingFace Embeddings**: Converts text to vectors (384 dimensions)
- **Ollama**: Local LLM for generating responses (`qwen3:4b-instruct`)
- **FastAPI**: Web server and API endpoints
- **React**: Frontend user interface

---

## ğŸ—ï¸ Architecture Components

### Backend Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FastAPI    â”‚â”€â”€â”€â–¶â”‚  RAG Chain   â”‚â”€â”€â”€â–¶â”‚   Ollama     â”‚  â”‚
â”‚  â”‚  (app.py)    â”‚    â”‚              â”‚    â”‚   (Local     â”‚  â”‚
â”‚  â”‚              â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚    LLM)      â”‚  â”‚
â”‚  â”‚  API Server  â”‚    â”‚  â”‚Retrieverâ”‚ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚                     â”‚
â”‚         â”‚             â”‚       â”‚       â”‚                     â”‚
â”‚         â”‚             â”‚       â–¼       â”‚                     â”‚
â”‚         â”‚             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                     â”‚
â”‚         â”‚             â”‚  â”‚ Pinecone â”‚  â”‚                     â”‚
â”‚         â”‚             â”‚  â”‚  Index   â”‚  â”‚                     â”‚
â”‚         â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                     â”‚
â”‚         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                      â”‚                              â”‚
â”‚         â”‚                      â”‚                              â”‚
â”‚         â–¼                      â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚                              â”‚
â”‚  â”‚  Embeddings  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚  â”‚  (HuggingFaceâ”‚                                              â”‚
â”‚  â”‚   all-MiniLM)â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Docs  â”‚â”€â”€â”€â–¶â”‚  store_index â”‚â”€â”€â”€â–¶â”‚   Pinecone   â”‚
â”‚  (data/)    â”‚    â”‚     .py      â”‚    â”‚   Vector DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Embeddings  â”‚
                  â”‚  Generation  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Workflow Phases

The complete workflow has **two main phases**:

### Phase 1: Document Processing & Indexing (One-Time Setup)
**Script:** `src/store_index.py`

### Phase 2: Query Processing & Answer Generation (Runtime)
**Script:** `app.py`

---

## ğŸ“š Phase 1: Document Processing & Indexing

This phase runs once (or when adding new documents) to process PDFs and create the vector database.

### Step-by-Step Process

#### Step 1: Load PDF Documents
**File:** `src/helper.py` â†’ `load_pdf_file()`

```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader

def load_pdf_file(data):
    loader = DirectoryLoader(data, glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    return documents
```

**What happens:**
- Scans `data/` directory for all `.pdf` files
- Uses `PyPDFLoader` to extract text from each PDF
- Returns a list of `Document` objects
- Each document contains:
  - `page_content`: Text content
  - `metadata`: Source file path, page numbers, etc.

**Example Output:**
```
[
  Document(page_content="Acne is a skin condition...", metadata={"source": "data/Medical_book.pdf", "page": 1}),
  Document(page_content="Diabetes is a metabolic...", metadata={"source": "data/Medical_book.pdf", "page": 2}),
  ...
]
```

#### Step 2: Filter Metadata
**File:** `src/helper.py` â†’ `filter_to_minimal_docs()`

```python
from typing import List
from langchain_core.documents import Document

def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:
    minimal_docs = []
    for doc in docs:
        src = doc.metadata.get("source")
        minimal_docs.append(
            Document(page_content=doc.page_content, metadata={"source": src})
        )
    return minimal_docs
```

**What happens:**
- Removes unnecessary metadata (page numbers, etc.)
- Keeps only the `source` (file path) in metadata
- Reduces storage size and simplifies processing

**Why:** Cleaner metadata reduces noise and improves vector search performance.

#### Step 3: Split into Chunks
**File:** `src/helper.py` â†’ `text_split()`

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

def text_split(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # Each chunk ~500 characters
        chunk_overlap=20     # 20 chars overlap between chunks
    )
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks
```

**What happens:**
- Splits long documents into smaller chunks (500 characters each)
- Uses 20-character overlap to preserve context at boundaries
- Creates many smaller documents from original PDFs

**Example:**
```
Original Document (2000 chars):
"[...] Acne is a common skin condition that affects millions of people worldwide. It occurs when hair follicles become clogged with oil and dead skin cells. The condition can range from mild to severe, with symptoms including whiteheads, blackheads, pimples, and in severe cases, cysts or nodules. Treatment options vary based on severity [...]"

After Splitting (4 chunks):
Chunk 1: "[...] Acne is a common skin condition that affects millions of people worldwide. It occurs when hair follicles become clogged with oil and dead skin cells. The condition can range from mild to severe, with symptoms including [...]"

Chunk 2: "[...] from mild to severe, with symptoms including whiteheads, blackheads, pimples, and in severe cases, cysts or nodules. Treatment options vary based on severity [...]"
```

**Why chunks?**
- LLMs have token limits
- Smaller chunks = more precise retrieval
- Overlap preserves context across boundaries

#### Step 4: Initialize Embedding Model
**File:** `src/helper.py` â†’ `download_hugging_face_embeddings()`

```python
from langchain_huggingface import HuggingFaceEmbeddings

def download_hugging_face_embeddings():
    embeddings = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2'
    )
    return embeddings
```

**What happens:**
- Downloads/loads the embedding model from HuggingFace
- Model: `all-MiniLM-L6-v2`
- Dimensions: **384** (each text becomes a 384-number vector)
- First-time run downloads the model (~80MB)

**How embeddings work:**
- Converts text into numerical vectors
- Semantically similar texts â†’ similar vectors
- Example:
  ```
  "What is acne?" â†’ [0.12, -0.45, 0.89, ..., 0.34]  (384 numbers)
  "Tell me about acne" â†’ [0.11, -0.44, 0.88, ..., 0.33]  (very similar!)
  "What is diabetes?" â†’ [0.78, 0.12, -0.23, ..., 0.67]  (different!)
  ```

#### Step 5: Create/Connect to Pinecone Index
**File:** `src/store_index.py` (lines 29-43)

```python
index_name = "medical-chatbot"

if not pc.has_index(index_name):
    pc.create_index(
        name=index_name,
        dimension=384,          # Must match embedding dimension
        metric="cosine",       # Similarity metric
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
```

**What happens:**
- Checks if index exists in Pinecone
- If not, creates a new index:
  - **Name:** `medical-chatbot`
  - **Dimensions:** 384 (matches embedding model)
  - **Metric:** Cosine similarity (measures angle between vectors)
  - **Cloud:** AWS us-east-1

**Why 384 dimensions?** Matches the output of `all-MiniLM-L6-v2` embedding model.

#### Step 6: Generate Embeddings & Upload to Pinecone
**File:** `src/store_index.py` (lines 78-83)

```python
from langchain_community.vectorstores import Pinecone as PineconeVectorStore

docsearch = PineconeVectorStore.from_documents(
    documents=text_chunks,     # All document chunks
    index_name=index_name,     # "medical-chatbot"
    embedding=embeddings       # HuggingFace model
)
```

**What happens:**
1. For each text chunk:
   - Converts text â†’ embedding vector (384 numbers)
   - Stores in Pinecone with:
     - **ID:** Auto-generated unique ID
     - **Vector:** The 384-dimension embedding
     - **Metadata:** Original text + source file path

2. Uploads all vectors to Pinecone cloud database

**Example in Pinecone:**
```
ID: "chunk_001"
Vector: [0.12, -0.45, 0.89, ..., 0.34]  (384 numbers)
Metadata: {
  "text": "Acne is a common skin condition...",
  "source": "data/Medical_book.pdf"
}
```

**Result:** Pinecone now contains thousands of indexed document chunks, searchable by semantic similarity.

---

## ğŸ” Phase 2: Query Processing & Answer Generation

This phase runs every time a user asks a question via the chatbot.

### Step-by-Step Process

#### Step 1: User Sends Question
**Frontend:** `static/app.js`

```javascript
const response = await fetch('/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message: userMessage.text })
});
```

**What happens:**
- User types question in React UI
- Frontend sends POST request to `/api/chat`
- Question: `"What is acne?"`

#### Step 2: FastAPI Receives Request
**File:** `app.py` (lines 80-97)

```python
@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    response = rag_chain.invoke({"input": request.message})
    return ChatResponse(answer=response["answer"])
```

**What happens:**
- FastAPI receives POST request
- Extracts `message` from JSON body
- Passes to `rag_chain` for processing

#### Step 3: Initialize RAG Chain Components
**File:** `app.py` (lines 35-63)

```python
# 1. Load embedding model (same as Phase 1)
embeddings = download_hugging_face_embeddings()

# 2. Connect to existing Pinecone index (READ-ONLY)
from langchain_community.vectorstores import Pinecone

docsearch = Pinecone.from_existing_index(
    index_name="medical-chatbot",
    embedding=embeddings
)

# 3. Create retriever (searches Pinecone)
retriever = docsearch.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # Get top 3 most similar chunks
)

# 4. Initialize Ollama LLM
chatModel = ChatOllama(model="qwen3:4b-instruct")

# 5. Create prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),  # Instructions for AI
    ("human", "{input}")        # User question
])

# 6. Build RAG chain using LCEL (LangChain Expression Language)
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def create_rag_chain_input(user_input: str):
    docs = retriever.invoke(user_input)
    return {"context": format_docs(docs), "input": user_input}

rag_chain = (
    RunnablePassthrough() | create_rag_chain_input
) | prompt | chatModel | StrOutputParser()
```

**Component Explanation:**

**A. Embeddings:**
- Same HuggingFace model as Phase 1
- Converts user question â†’ vector for search

**B. Retriever:**
- Searches Pinecone for similar documents
- `k=3`: Returns top 3 most relevant chunks
- `search_type="similarity"`: Uses cosine similarity

**C. Ollama LLM:**
- Local language model (`qwen3:4b-instruct`)
- Generates natural language answers
- Runs on your machine (privacy-preserving)

**D. Prompt Template:**
- **System prompt:** Instructions telling AI it's a medical assistant
- **Human input:** The user's question

**E. RAG Chain (LCEL):**
- Uses LangChain Expression Language (LCEL) with pipe operators (`|`)
- Retrieves documents â†’ formats context â†’ builds prompt â†’ generates answer
- Modern, composable approach to building chains

#### Step 4: RAG Chain Processing
**When:** `rag_chain.invoke(request.message)` is called

**Sub-step 4a: Convert Question to Embedding**

```
Question: "What is acne?"
         â†“
Embedding Model
         â†“
Vector: [0.12, -0.45, 0.89, ..., 0.34]
```

**Sub-step 4b: Search Pinecone**

```
Question Vector â†’ Pinecone Index
                      â†“
                 Similarity Search
                      â†“
        Find Top 3 Most Similar Chunks
                      â†“
Retrieved Documents:
1. "Acne is a common skin condition that affects..."
2. "Symptoms of acne include whiteheads, blackheads..."
3. "Treatment options for acne vary based on severity..."
```

**How similarity search works:**
- Calculates cosine similarity between question vector and all stored vectors
- Cosine similarity = angle between vectors (0 to 1)
- Higher similarity = more relevant
- Returns top 3 matches

**Sub-step 4c: Prepare Context for LLM**

```
Retrieved Documents + Question
         â†“
System Prompt + Context + Question
         â†“
Full Prompt:
"""
You are a Medical assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, say that you don't know.
Use three sentences maximum and keep the answer concise.

Context:
1. Acne is a common skin condition that affects millions...
2. Symptoms of acne include whiteheads, blackheads, pimples...
3. Treatment options for acne vary based on severity...

Question: What is acne?
"""
```

**Sub-step 4d: Generate Answer with Ollama**

```
Full Prompt â†’ Ollama LLM (qwen3:4b-instruct)
                    â†“
            LLM Processing
                    â†“
           Generated Answer:
"Acne is a common skin disease characterized by pimples on the face, 
chest, and back. It occurs when the pores of the skin become clogged 
with oil, dead skin cells, and bacteria. Acne vulgaris is the medical 
term for common acne and is the most common skin disease."
```

**How Ollama works:**
- Runs locally on your machine
- Takes prompt with context + question
- Generates natural language response
- Uses retrieved context to ensure accuracy

#### Step 5: Return Answer to Frontend

```python
return ChatResponse(answer=answer)
```

**What happens:**
- RAG chain returns string directly (via LCEL `StrOutputParser()`)
- FastAPI returns JSON response:
```json
{
  "answer": "Acne is a common skin disease characterized by..."
}
```
- Frontend displays answer in chat interface

---

## ğŸ”— Detailed Component Breakdown

### 1. Embedding Model (`all-MiniLM-L6-v2`)

**Purpose:** Convert text to numerical vectors for semantic search

**Characteristics:**
- **Dimensions:** 384
- **Size:** ~80MB
- **Model Type:** Sentence transformer
- **Provider:** HuggingFace

**How it works:**
- Trained on millions of text pairs
- Understands semantic meaning
- Similar meanings â†’ similar vectors
- Imported from `langchain_huggingface` package

**Example:**
```
Text: "What is acne?"
â†’ Vector: [0.12, -0.45, 0.89, 0.23, ..., -0.67]

Text: "Tell me about acne"
â†’ Vector: [0.11, -0.44, 0.88, 0.24, ..., -0.66]
         (Very similar - cosine similarity â‰ˆ 0.98)

Text: "What is diabetes?"
â†’ Vector: [0.78, 0.12, -0.23, 0.45, ..., 0.34]
         (Different - cosine similarity â‰ˆ 0.15)
```

### 2. Pinecone Vector Database

**Purpose:** Store and search document embeddings at scale

**Characteristics:**
- **Cloud-hosted:** AWS us-east-1
- **Index Name:** `medical-chatbot`
- **Dimension:** 384
- **Metric:** Cosine similarity
- **Storage:** Thousands of document chunks
- **Import:** `from langchain_community.vectorstores import Pinecone`

**Operations:**
- **Write:** During `store_index.py` (Phase 1) - uses `Pinecone.from_documents()`
- **Read:** During `app.py` queries (Phase 2) - uses `Pinecone.from_existing_index()`

**Why Pinecone?**
- Fast similarity search (milliseconds)
- Scalable (millions of vectors)
- Managed service (no infrastructure management)

### 3. Retriever (`as_retriever`)

**Purpose:** Find relevant documents from Pinecone

**Configuration:**
```python
retriever = docsearch.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)
```

**Parameters:**
- `search_type="similarity"`: Uses cosine similarity
- `k=3`: Returns top 3 most relevant chunks

**Process:**
1. User question â†’ embedding vector
2. Search Pinecone for similar vectors
3. Return top 3 matching chunks

**Why k=3?**
- Balance between context and token limits
- Too few (k=1): Might miss relevant info
- Too many (k=10): Overwhelms LLM, slow response

### 4. Ollama LLM (`qwen3:4b-instruct`)

**Purpose:** Generate natural language answers

**Characteristics:**
- **Model:** Qwen3 4B Instruct
- **Size:** ~4 billion parameters
- **Type:** Instruction-tuned (follows prompts well)
- **Location:** Local (runs on your machine)

**Why local?**
- Privacy: Data never leaves your machine
- Cost: Free (no API costs)
- Control: Full control over model and data

**How it works:**
1. Receives prompt with:
   - System instructions
   - Retrieved context (3 document chunks)
   - User question
2. Generates answer based on context
3. Returns concise, accurate response

### 5. Prompt Template

**File:** `src/prompt.py`

```python
system_prompt = (
    "You are an Medical assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)
```

**Purpose:** Guide LLM behavior

**Key Instructions:**
- Role: Medical assistant
- Behavior: Use retrieved context
- Honesty: Say "don't know" if unsure
- Format: 3 sentences max, concise

### 6. LangChain Chains

**Components:**

**A. LCEL Chain Composition:**
- Uses pipe operators (`|`) to compose chain components
- `RunnablePassthrough()`: Passes input through
- `create_rag_chain_input`: Formats retrieved documents into context
- `prompt`: Formats prompt with context and question
- `chatModel`: Generates answer
- `StrOutputParser()`: Parses output to string

**Flow:**
```
Question â†’ Retrieval â†’ Format Context â†’ Prompt â†’ LLM â†’ Parse â†’ Answer
```

---

## ğŸ“Š Data Flow Diagrams

### Complete Flow: Question to Answer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INTERACTION                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  React Frontend â”‚
                   â”‚  (app.js)       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ POST /api/chat
                            â”‚ {message: "What is acne?"}
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   FastAPI Server (app.py)    â”‚
              â”‚   /api/chat endpoint         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    RAG Chain Processing      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 1: Question â†’ Embedding          â”‚
        â”‚  "What is acne?" â†’ [0.12, -0.45, ...] â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 2: Search Pinecone               â”‚
        â”‚  Find top 3 similar chunks             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 3: Retrieve Context               â”‚
        â”‚  Chunk 1: "Acne is a common..."        â”‚
        â”‚  Chunk 2: "Symptoms include..."        â”‚
        â”‚  Chunk 3: "Treatment options..."       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 4: Build Prompt                  â”‚
        â”‚  System: "You are a medical..."        â”‚
        â”‚  Context: [Chunk 1, 2, 3]              â”‚
        â”‚  Question: "What is acne?"             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  STEP 5: Generate Answer (Ollama)      â”‚
        â”‚  LLM processes prompt â†’ Answer          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Return JSON       â”‚
              â”‚  {answer: "..."}   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Frontend  â”‚
                   â”‚  Display   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Document Indexing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT PROCESSING FLOW                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Files   â”‚  "Medical_book.pdf"
â”‚  (data/)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  load_pdf_file()       â”‚  Extract text from PDFs
â”‚  PyPDFLoader           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  filter_to_minimal_    â”‚  Clean metadata
â”‚  docs()                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  text_split()          â”‚  Split into 500-char chunks
â”‚  RecursiveCharacter    â”‚  (with 20-char overlap)
â”‚  TextSplitter          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚  Example: 1 PDF â†’ 100 chunks
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  download_hugging_     â”‚  Load embedding model
â”‚  face_embeddings()     â”‚  all-MiniLM-L6-v2
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For each chunk:       â”‚
â”‚  1. Text â†’ Embedding   â”‚  Convert to 384-dim vector
â”‚  2. Upload to Pinecone â”‚  Store with metadata
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pinecone Index        â”‚  Thousands of indexed chunks
â”‚  medical-chatbot       â”‚  Ready for semantic search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Code Execution Flow

### Phase 1: Setup (`store_index.py`)

```python
# 1. Load environment variables
load_dotenv()

# 2. Extract PDFs
extracted_data = load_pdf_file(data='data/')      # Load PDFs
filter_data = filter_to_minimal_docs(...)         # Clean metadata
text_chunks = text_split(...)                     # Split into chunks

# 3. Initialize embeddings
embeddings = download_hugging_face_embeddings()   # Load model

# 4. Setup Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)           # Connect
if not pc.has_index("medical-chatbot"):          # Check index
    pc.create_index(...)                           # Create if needed

# 5. Upload documents
from langchain_community.vectorstores import Pinecone as PineconeVectorStore
PineconeVectorStore.from_documents(...)           # Generate embeddings & upload
```

### Phase 2: Runtime (`app.py`)

```python
# Application startup (runs once)
embeddings = download_hugging_face_embeddings()        # Load embedding model
from langchain_community.vectorstores import Pinecone
docsearch = Pinecone.from_existing_index(...)          # Connect to Pinecone
retriever = docsearch.as_retriever(k=3)                # Setup retriever
chatModel = ChatOllama(model="qwen3:4b-instruct")      # Load LLM
# Build RAG chain using LCEL (see code for details)
rag_chain = (RunnablePassthrough() | ...) | prompt | chatModel | StrOutputParser()

# Per-request processing (runs for each question)
@app.post("/api/chat")
async def chat(request: ChatRequest):
    answer = rag_chain.invoke(request.message)         # Process question (returns string)
    return ChatResponse(answer=answer)                   # Return answer
```

### Phase 2 Internal: RAG Chain Execution

```
rag_chain.invoke("What is acne?")
    â”‚
    â”œâ”€â†’ RunnablePassthrough() passes input through
    â”‚
    â”œâ”€â†’ create_rag_chain_input("What is acne?")
    â”‚   â”‚
    â”‚   â”œâ”€â†’ retriever.invoke("What is acne?")
    â”‚   â”‚   â”œâ”€â†’ Convert "What is acne?" â†’ embedding vector
    â”‚   â”‚   â”œâ”€â†’ Search Pinecone with vector
    â”‚   â”‚   â””â”€â†’ Return top 3 similar chunks: [doc1, doc2, doc3]
    â”‚   â”‚
    â”‚   â””â”€â†’ Return {"context": format_docs([doc1, doc2, doc3]), "input": "What is acne?"}
    â”‚
    â”œâ”€â†’ prompt.invoke({context, input})
    â”‚   â”œâ”€â†’ Format prompt:
    â”‚   â”‚   - System: "You are a medical assistant..."
    â”‚   â”‚   - Context: doc1 + doc2 + doc3
    â”‚   â”‚   - Question: "What is acne?"
    â”‚
    â”œâ”€â†’ chatModel.invoke(formatted_prompt)
    â”‚   â””â”€â†’ Ollama generates answer
    â”‚
    â””â”€â†’ StrOutputParser() parses to string
        â†’ "Acne is a common skin disease..."
```

---

## ğŸ”‘ Key Concepts

### 1. Embeddings
- **What:** Numerical representations of text
- **Why:** Enable semantic search (finding meaning, not just keywords)
- **How:** Neural network transforms text â†’ vector of numbers

### 2. Vector Similarity
- **Cosine Similarity:** Measures angle between vectors (0-1 scale)
- **1.0:** Identical meaning
- **0.0:** Completely different
- **0.7+:** Semantically related

### 3. RAG (Retrieval Augmented Generation)
- **Retrieval:** Find relevant documents
- **Augmentation:** Add context to prompt
- **Generation:** LLM creates answer with context
- **Benefit:** More accurate than LLM alone (uses actual documents)

### 4. Chunking
- **Why:** LLMs have token limits, smaller chunks = precise retrieval
- **Strategy:** 500 chars with 20-char overlap
- **Trade-off:** Balance between context and precision

---

## ğŸ“ˆ Performance Considerations

### Embedding Generation
- **Time:** ~0.1-1 second per chunk
- **Bottleneck:** First-time model download (~80MB)
- **Optimization:** Model cached after first use

### Pinecone Search
- **Time:** ~50-200ms per query
- **Bottleneck:** Network latency to cloud
- **Scalability:** Handles millions of vectors efficiently

### LLM Generation (Ollama)
- **Time:** ~2-10 seconds per answer
- **Bottleneck:** Local CPU/GPU performance
- **Optimization:** Use GPU if available

### Total Query Time
- **Typical:** 3-12 seconds end-to-end
- **Breakdown:**
  - Embedding: ~0.1s
  - Pinecone search: ~0.2s
  - LLM generation: ~2-10s

---

## ğŸ“ Summary

1. **Setup Phase (`store_index.py`):**
   - Process PDFs â†’ Chunks â†’ Embeddings â†’ Pinecone
   - Run once or when adding documents
   - Uses `langchain_huggingface` for embeddings
   - Uses `langchain_community.vectorstores.Pinecone` for storage

2. **Runtime Phase (`app.py`):**
   - Question â†’ Embedding â†’ Search Pinecone â†’ Retrieve context â†’ LLM â†’ Answer
   - Runs for every user query
   - Uses LCEL (LangChain Expression Language) for chain composition

3. **Key Technologies:**
   - **Embeddings:** `langchain_huggingface.HuggingFaceEmbeddings` for semantic understanding
   - **Pinecone:** `langchain_community.vectorstores.Pinecone` for fast vector search
   - **Ollama:** Local LLM generation via `langchain_ollama.ChatOllama`
   - **LangChain:** Orchestration using LCEL pattern

4. **Benefits of RAG:**
   - Accurate (uses actual documents)
   - Transparent (can trace sources)
   - Updatable (add new documents anytime)
   - Privacy-preserving (local LLM)

---

**ğŸ‰ This completes the complete workflow documentation!**

For questions or clarifications, refer to the individual code files or the main README.md.

